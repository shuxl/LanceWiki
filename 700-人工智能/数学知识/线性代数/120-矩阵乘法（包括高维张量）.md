# 1 矩阵乘法详解：从基础到高维张量

矩阵乘法是线性代数中的核心运算，在PyTorch中通过`torch.matmul()`或`@`运算符实现。下面我将从基础数学公式开始，逐步扩展到高维张量场景。

---

## 1.1 ​**1. 二维矩阵乘法（标准矩阵乘法）​**​

​**数学定义**​：  
对于矩阵 $A \in \mathbb{R}^{m \times n}$ 和 $B \in \mathbb{R}^{n \times p}$，  
它们的乘积 $C = A \times B$ 定义为：

$$
C_{ij} = \sum_{k=1}^{n} A_{ik} \times B_{kj}
$$

其中 $C \in \mathbb{R}^{m \times p}$，即：

- 结果矩阵的行数 = A的行数
- 结果矩阵的列数 = B的列数
- ​**关键约束**​：A的列数必须等于B的行数（即维度n必须匹配）

​**图示**​：

```
     A (m×n)   ×   B (n×p)   =   C (m×p)
    ┌─────────┐   ┌─────────┐   ┌─────────┐
    │ a₁₁ ... │   │ b₁₁ ... │   │ ∑a₁ₖbₖ₁ ... │
    │ ... ... │   │ ... ... │   │ ...     ... │
    │ aₘ₁ ... │   │ bₙ₁ ... │   │ ∑aₘₖbₖₚ     │
    └─────────┘   └─────────┘   └─────────┘
```

​**PyTorch示例**​：

``` python
A = torch.tensor([[1, 2], [3, 4]])  # 2×2
B = torch.tensor([[5, 6], [7, 8]])  # 2×2
C = A @ B
#[[1 * 5 + 2 * 7, 1 * 6 + 2 * 8] = [[19, 22]
# [3 * 5 + 4 * 7, 3 * 6 + 4 * 8]]   [43, 50]]
```

---

## 1.2 ​**2. 高维张量乘法（批处理矩阵乘法）​**​

PyTorch的`torch.matmul()`支持高维张量，遵循**广播机制**​：

- ​**核心规则**​：只对最后两个维度做矩阵乘法，前面维度视为批处理维度
- ​**数学表示**​：  
    对于张量 $A \in \mathbb{R}^{b_1 \times ... \times b_k \times m \times n}$ 和  
    $B \in \mathbb{R}^{b_1 \times ... \times b_k \times n \times p}$
    结果 $C \in \mathbb{R}^{b_1 \times ... \times b_k \times m \times p}$  
    其中每个"切片"满足：
    
    $$
    C_{...,i,j} = \sum_{k=1}^{n} A_{...,i,k} \times B_{...,k,j}
    $$
    

### 1.2.1 ​**规则详解**​：

1. ​**维度对齐**​：从右向左对齐维度
    
    ```
    A: (..., m, n)
    B: (..., n, p)
    C: (..., m, p)
    ```
    
2. ​**广播规则**​：
    - 前导维度（批处理维度）必须满足广播条件
    - 最后两个维度必须满足矩阵乘法条件（A的最后一维 = B的倒数第二维）

---

## 1.3 ​**3. 典型高维场景示例**​

### 1.3.1 ​**场景1：向量与矩阵相乘**​

``` python
v = torch.tensor([1, 2])       # 向量 (2,)
M = torch.tensor([[3, 4], [5, 6]])  # 矩阵 (2,2)

# 向量在左侧：自动视为行向量 (1×2)
print(v @ M)  # = [1,2] @ [[3,4],[5,6]] = [1 * 3+2 * 5, 1 * 4+2 * 6] = [13, 16]

# 向量在右侧：自动视为列向量 (2×1)
print(M @ v)  # = [[3,4],[5,6]] @ [1,2] = [3 * 1+4 * 2, 5 * 1+6 * 2] = [11, 17]
```

### 1.3.2 ​**场景2：批量矩阵乘法（相同批尺寸）​**​

```
A = torch.randn(3, 2, 4)  # 3个2×4矩阵
B = torch.randn(3, 4, 5)  # 3个4×5矩阵
C = A @ B  # 结果形状 (3, 2, 5)
```

### 1.3.3 ​**场景3：广播批处理维度**​

```
A = torch.randn(2, 1, 3, 4)  # 形状 (2,1,3,4)
B = torch.randn(  4, 4, 5)    # 形状 (4,4,5) -> 广播为 (1,4,4,5)

C = A @ B  # 计算过程：
# 1. 广播对齐：A(2,1,3,4) + B(1,4,4,5) → A(2,4,3,4) + B(2,4,4,5)
# 2. 逐批矩阵乘：(3,4) × (4,5) → (3,5)
# 结果形状: (2,4,3,5)
```

### 1.3.4 ​**场景4：高维复杂广播**​

```
A = torch.randn(5, 1, 3, 4)  # (5,1,3,4)
B = torch.randn(3, 4, 2)      # (3,4,2) -> 广播为 (1,3,4,2)

C = A @ B  # 计算过程：
1. 广播对齐：A(5,1,3,4) + B(1,3,4,2) → A(5,3,3,4) + B(5,3,4,2)
2. 逐批矩阵乘：(3,4) × (4,2) → (3,2)
结果形状: (5,3,3,2)
```

---

## 1.4 ​**4. 特殊边界情况**​

### 1.4.1 ​**标量乘法（退化情况）​**​

```
s = torch.tensor(2.0)  # 标量 ()
M = torch.tensor([[1,2],[3,4]])
print(s @ M)  # 等价于标量乘法 [[2,4],[6,8]]
```

### 1.4.2 ​**维度不匹配错误**​

```
A = torch.randn(2,3,4)
B = torch.randn(2,5,4)  # 错误！最后两维应为(4,?)和(?,?)

正确做法1：调整维度
B_adj = B.transpose(1,2)  # (2,4,5)
A @ B_adj  # 合法：(2,3,4) × (2,4,5) → (2,3,5)

正确做法2：使用广播
C = torch.randn(3,4)     # (3,4)
A @ C                   # 广播为 (2,3,4) × (3,4) → (2,3,4)? 
错误！需要显式扩展：
C_exp = C.unsqueeze(0).expand(2,3,4)  # (2,3,4)
A @ C_exp.transpose(1,2)  # (2,3,4) × (2,4,?) → 维度仍不匹配
```

---

## 1.5 ​**5. 与相关函数的区别**​

|​**函数**​|​**适用场景**​|​**维度约束**​|
|---|---|---|
|`torch.matmul()`|通用矩阵乘法（支持高维广播）|最后两维需满足矩阵乘法规则|
|`torch.mm()`|严格二维矩阵乘法|必须都是二维|
|`torch.bmm()`|批量矩阵乘法（无广播）|必须都是三维且第一维相同|
|`torch.einsum()`|爱因斯坦求和约定（最灵活）|自定义维度规则|
|`A * B`|逐元素乘法（Hadamard积）|需满足广播规则|

---

# 2 ​**总结：高维张量乘法核心规则**​

1. ​**最后两个维度**​：必须严格满足矩阵乘法条件  
    `(..., m, n) × (..., n, p) → (..., m, p)`
    
2. ​**前导维度**​：必须满足广播规则
    
    - 从右向左对齐维度
    - 每个维度需满足：相等/其中一个为1/其中一个不存在
3. ​**自动广播**​：PyTorch自动扩展维度
    
    ```
    # 自动广播示例
    A = torch.randn(2,3,4)    # (2,3,4)
    B = torch.randn(  4,5)    # (4,5) → 广播为(1,4,5) → (2,4,5)
    C = A @ B                # (2,3,4) × (2,4,5) → (2,3,5)
    ```
    

掌握这些规则对于理解神经网络中的全连接层、注意力机制等核心操作至关重要。当维度复杂时，建议使用`torch.einsum()`明确指定计算规则。