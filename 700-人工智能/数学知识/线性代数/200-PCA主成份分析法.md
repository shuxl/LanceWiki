#算法/PCA

+ 主要理解关键特征
+  协方差矩阵：
	+ [210-协方差矩阵](210-协方差矩阵.md)
		+ [211-协方差矩阵反映了原始特征之间的线性关系](211-协方差矩阵反映了原始特征之间的线性关系.md)
		+ [212-标准化后的协方差矩阵分析](212-标准化后的协方差矩阵分析.md)
+  特征值、特征向量
	+ [213-求解矩阵方程的特征值和特征向量](213-求解矩阵方程的特征值和特征向量.md)
		+ [214-矩阵的行列式](214-矩阵的行列式.md)
		+ [215-矩阵行列式的推导过程（2x2）](215-矩阵行列式的推导过程（2x2）.md)
+  构建投影矩阵
	+ [216-PCA中的投影逻辑](216-PCA中的投影逻辑.md)
		+ [217-向量的点积](217-向量的点积.md)
		+ [218-单位向量](218-单位向量.md)
		+ [219-PCA中投影的数学本质和主成分数量与输出数据形状的关系](219-PCA中投影的数学本质和主成分数量与输出数据形状的关系.md)
# 1 PCA 简介

**一句话概括：**
​**PCA是一种强大的统计方法，用于通过线性变换将高维数据降维到低维空间（称为“主成分”），同时尽可能保留原始数据中的主要信息和变化模式。​**​
## 1.1 核心思想与目的
​
1. ​**降维（Dimensionality Reduction）：​**​ 现实世界的数据（如图像、基因数据、金融指标、用户行为数据）往往包含大量特征（维度）。这些特征：
    - ​**冗余：​**​ 很多特征可能是相关的（比如人的身高和体重通常高度相关）。
    - ​**噪声：​**​ 部分维度可能包含噪声或不重要信息。
    - ​**计算负担：​**​ 高维数据会导致计算复杂、存储开销大（“维度灾难”）。
    - ​**可视化困难：​**​ 我们很难直观地理解或可视化三维以上的数据。
    
    ​**PCA的目标就是找到比原始维度少得多的一组新特征（主成分），用这些新特征能尽可能好地“代表”原始数据。​**​
    
2. ​**信息浓缩与特征提取：​**​
    - 主成分是原始特征的线性组合（加权平均）。
    - 第一个主成分 ​**方向**​ 是原始数据在_投影_后**方差最大**的方向。方差代表了信息量，因此它捕捉了数据中最多的变化/信息。
    - 第二个主成分方向是与第一主成分**正交（垂直）​**，且在剩余方向中**方差最大**的方向（即捕捉第一主成分未能解释的主要变化）。
    - 后续主成分依次类推，每个都与之前的主成分正交，并尽量捕捉剩余的方差。
    - 这些新的主成分变量之间是**相互独立（不相关）​**的。

## 1.2 关键特性
- ​**线性变换：​**​ PCA 是线性的，新变量（主成分）是原始变量的线性组合。
- ​**最大化方差：​**​ 投影方向的选择目标是最大化投影后数据的方差。
- ​**特征重要性排序：​**​ 每个主成分对应一个**特征值**​（Eigenvalue）。特征值的大小表示该主成分所解释的原始数据**方差的比例**。通常主成分按其特征值从大到小排序，这意味着第一个主成分包含信息最多，第二个次之，以此类推。
- ​**降维选择：​**​ 通常我们只保留前 $k$ 个主成分（$k$ 远小于原始特征数 $d$）。选择 $k$ 的方法可以基于：
    - ​**累积方差贡献率：​**​ 选择累积贡献率达到某个阈值（如80%、90%）的前k个主成分。
    - ​**碎石图（Scree Plot）：​**​ 观察特征值下降的“拐点”，保留拐点之前的主成分。
- ​**去相关：​**​ 主成分之间**互不相关（协方差为零）​**，消除了原始特征之间的相关性。

# 2 PCA的基本步骤：​
​
1. ​**标准化数据（中心化+缩放）：​**​ 将所有特征减去其均值（中心化），并通常除以其标准差（缩放）。这一步确保不同尺度的特征被公平对待。
2. ​**计算协方差矩阵（或相关矩阵）：​**​ 协方差矩阵反映了原始特征之间的线性关系。
3. ​**计算特征值和特征向量：​**​
    - 对协方差矩阵进行**特征分解**​（Eigenvalue Decomposition）。
    - 得到特征值（λ₁ ≥ λ₂ ≥ … ≥ λd）和对应的特征向量（v₁, v₂, …, vd）。
    - 每个特征向量代表一个主成分的方向，其对应的特征值代表该方向上的方差。
4. ​**选择主成分：​**​ 选择前 $k$ 个最大的特征值对应的特征向量（通常按累积方差贡献率或碎石图）。
5. ​**构建投影矩阵：​**​ 将选中的前 $k$ 个特征向量按列堆叠，形成一个投影矩阵。
6. ​**转换数据：​**​ 将原始（标准化后的）数据点 $X$ 乘以上述投影矩阵 $P$，得到降维后的数据 $Y = X.P$。$Y$ 的每一行对应一个样本的低维表示（由k个主成分得分组成）。

# 3 主要应用：​ 

- ​**数据可视化：​**​ 将高维数据降到2维或3维进行可视化。
- ​**特征提取：​**​ 从大量原始特征中提取少数几个最具代表性的综合特征，用于后续的机器学习建模。
- ​**数据压缩：​**​ 减少数据存储和计算开销。
- ​**消除特征相关性（去相关）：​**​ 为一些假设输入独立的算法（如朴素贝叶斯）做预处理。
- ​**噪声去除：​**​ 较小的主成分可能主要代表噪声，可以舍弃它们。
- ​**探索性数据分析：​**​ 理解数据中主要的变异来源。

# 4 需要注意的局限：​

- ​**线性假设：​**​ PCA只能发现线性结构。对于具有复杂非线性关系的数据，效果可能不好，此时可能需要非线性降维方法（如t-SNE, UMAP）。
- ​**可解释性降低：​**​ 主成分是原始特征的线性组合，物理含义可能不如原始特征直观（尽管可以分析其载荷来尝试理解）。
- ​**主成分方向取决于尺度：​**​ 标准化数据非常重要，否则结果会受到量纲大的特征主导。
- ​**可能丢失信息：​**​ 舍弃的主成分也可能包含一些重要的信息（特别是当数据本质就是高维结构时）。

# 5 总结说：​​

PCA是一种通过寻找数据中方差最大的正交方向来降低数据维度的有效方法。它帮助我们在减少数据量的同时，最大限度地保留最重要的信息，是数据分析、机器学习和数据科学中广泛使用的基础工具。  