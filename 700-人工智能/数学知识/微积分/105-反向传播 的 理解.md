**反向传播**（Backpropagation，简称 **BP**）是神经网络训练中用来**计算每个参数的梯度**的一种高效算法，是现代深度学习的基础之一。

它的作用可以简要总结为：
> **用链式法则高效计算损失函数对每个权重的导数，然后用这些导数来更新参数。**


+ 知识点
	+  [107-损失函数](107-损失函数.md)
	+ $\partial$ 偏导数  [108-偏导数](108-偏导数.md)


下面我会结合机器学习背景，从**直观解释**、**数学过程**和**实际例子**几个方面逐步讲清楚反向传播是什么、为什么要用它，以及怎么实现。

---

# 1 **一、为什么需要反向传播？**

神经网络的训练目标是：**最小化损失函数**。

这个过程需要不断调整网络中的参数（权重 $\theta$ 和偏置 b），而参数更新的核心就是梯度下降：

$\theta \leftarrow \theta - \eta \cdot \frac{\partial L}{\partial \theta}$

所以关键在于：**如何高效地计算损失函数** L **对每一个参数的偏导数？**

在多层神经网络中，层数一多，函数嵌套就很深，比如：

$L = \text{loss}(f_3(f_2(f_1(x))))$

你不可能一个一个手推导，因此需要一个**系统化、自动化的微分算法** —— 这就是反向传播。

---

# 2 **二、反向传播做了什么？（直观理解）**

神经网络的训练过程包含两个阶段：
1. **前向传播（Forward Pass）**：
    - 从输入开始，一层一层地计算输出（最终得出预测值）。
    - 公式示例（每一层）：
        $z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} \\ a^{(l)} = \sigma(z^{(l)})$
2. **反向传播（Backward Pass）**：
    - 从输出层开始，**一层一层地反向计算梯度**，最终得到每个权重 $W^{(l)}$、偏置 $b^{(l)}$ 的导数 $\frac{\partial L}{\partial W^{(l)}}、\frac{\partial L}{\partial b^{(l)}}$。

> 本质就是把“链式法则”系统地应用到整个网络上。

---

# 3 **三、反向传播的数学原理（简要）**

对于输出层的误差：
$\delta^{(L)} = \frac{\partial L}{\partial a^{(L)}} \cdot \sigma’(z^{(L)})$
对于隐藏层的误差（递归计算）：
$\delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \cdot \sigma’(z^{(l)})$
然后计算梯度：
$\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T \\ \frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}$
这就是核心步骤。

---

# 4 **四、为什么要用链式法则？**

神经网络的层层嵌套结构其实就是一个复合函数：
$L = f^{(L)}(f^{(L-1)}( \cdots f^{(1)}(x)))$
所以要求最外层对最内层参数（比如第 1 层权重 W^{(1)}）的导数时，只能用**链式法则**：
$\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial f^{(L)}} \cdot \frac{\partial f^{(L)}}{\partial f^{(L-1)}} \cdots \frac{\partial f^{(2)}}{\partial f^{(1)}} \cdot \frac{\partial f^{(1)}}{\partial W^{(1)}}$
反向传播就是**系统地利用链式法则**从输出往输入方向高效地传播每一层的导数。

---

# 5 **五、例子（两层网络）**

假设一个简单神经网络：
- 输入 $x$
- 一层隐藏层：$z = W_1 x + b_1$, 激活 $a = \sigma(z)$
- 输出层：$\hat{y} = W_2 a + b_2$
- 损失函数：$L = \frac{1}{2}(y - \hat{y})^2$
训练目标：更新 $W_1, b_1, W_2, b_2$ 使得 $L$ 最小

**前向传播：**

1. 计算隐藏层 $a = \sigma(W_1 x + b_1)$
2. 计算输出 $\hat{y} = W_2 a + b_2$
3. 计算损失 $L = \frac{1}{2}(y - \hat{y})^2$

**反向传播：**
1. 误差从输出层开始传播
2. 逐层应用链式法则，计算各层参数的导数
3. 应用梯度下降更新参数

---

# 6 **六、反向传播的优点**

- **高效**：一遍前向 + 一遍反向，就能得到所有参数的梯度
- **通用**：适用于任意深度的网络
- **自动微分框架的基础**：比如 PyTorch、TensorFlow 内部就是自动执行反向传播

---

# 7 **七、总结**

|**项目**|**内容**|
|---|---|
|本质|用链式法则计算每层的导数|
|目的|得到损失函数对每个参数的梯度|
|地位|神经网络训练的核心算法|
|关键词|链式法则、误差传播、梯度计算|
