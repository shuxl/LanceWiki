这是一个非常重要的推导，尤其在**神经网络反向传播**和**逻辑回归梯度计算**中经常用到。我们现在一步步来推导这个公式：
$\sigma’(z) = \sigma(z)(1 - \sigma(z))$

其中 $\sigma(z)$ 是 **Sigmoid 函数**：

$\sigma(z) = \frac{1}{1 + e^{-z}}$

---

# 1 **一、目标**

我们要求的就是：
$\sigma’(z) = \frac{d}{dz} \left( \frac{1}{1 + e^{-z}} \right)$

---

# 2 **二、使用链式法则推导**
  
我们设：
$f(z) = \frac{1}{1 + e^{-z}}$
对这个函数求导，我们可以把它看成复合函数来求导。
记住两个导数公式：
1. $\frac{d}{dz}(e^{-z}) = -e^{-z}$
2. $\frac{d}{dz} \left( \frac{1}{g(z)} \right) = -\frac{g’(z)}{(g(z))^2}$
设：
$g(z) = 1 + e^{-z}$
那么：
$f(z) = \frac{1}{g(z)} \Rightarrow f’(z) = -\frac{g’(z)}{g(z)^2}$
先算 $g’(z)$：
$g’(z) = \frac{d}{dz} (1 + e^{-z}) = -e^{-z}$
所以：
$f’(z) = -\frac{-e^{-z}}{(1 + e^{-z})^2} = \frac{e^{-z}}{(1 + e^{-z})^2}$

---

# 3 **三、把结果写成** $\sigma(z)$ **的形式**

现在回顾一下：
$\sigma(z) = \frac{1}{1 + e^{-z}}$
所以：
$1 - \sigma(z) = 1 - \frac{1}{1 + e^{-z}} = \frac{e^{-z}}{1 + e^{-z}}$
我们现在有两个量：
- $\sigma(z) = \frac{1}{1 + e^{-z}}$
- $1 - \sigma(z) = \frac{e^{-z}}{1 + e^{-z}}$
将它们相乘：
$\sigma(z)(1 - \sigma(z)) = \left( \frac{1}{1 + e^{-z}} \right) \left( \frac{e^{-z}}{1 + e^{-z}} \right) = \frac{e^{-z}}{(1 + e^{-z})^2}$
这个结果就是我们刚刚推导出来的导数！

---

# 4 **四、结论**

所以：
$\sigma’(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \sigma(z)(1 - \sigma(z))$

---

# 5 **五、理解意义（直观上）**

- 当 $\sigma(z)$ 趋近于 0 或 1（也就是 z 很小或很大）时，它的导数趋近于 0；
- 导数最大时是在 $\sigma(z) = 0.5$ 的时候，最大值是 0.25；
- 所以 sigmoid 的导数呈“钟型”，这也是它在深层神经网络中容易导致**梯度消失**的原因。