# 1 开篇词
开篇大纲
![](img/Pasted%20image%2020250319163849.png)
# 2 数据基础
## 2.1 线性代数

而在向量和矩阵背后，线性代数的核心意义在于提供了⼀种看待世界的抽象视角：万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察。

线性代数中最基本的概念是集合（set）。
向量和矩阵不只是理论上的分析工具，也是计算机工作的基础条件。人类能够感知连续变化的大千世界，可计算机只能处理离散取值的二进制信息，因而来自模拟世界的信号必须在定义域和值域上同时进行数字化，才能被计算机存储和处理。从这个角度看，线性代数是用虚拟数字世界表示真实物理世界的工具。

![](img/Pasted%20image%2020250319164134.png)

在计算机存储中，标量占据的是零维数组；向量占据的是一维数组，例如语音信号；矩阵占据的是二维数组，例如灰度图像；张量占据的是三维乃至更高维度的数组，例如 RGB 图像和视频。
[小知识-灰度图像、彩色图像、视频所对应的张量结构](小知识-灰度图像、彩色图像、视频所对应的张量结构.md)

描述作为数学对象的向量需要有特定的数学语言，范数和内积就是代表。

## 2.2 概率论
概率论是线性代数之外，人工智能的另一个理论基础，多数机器学习模型采用的都是基于概率论的方法。但由于实际任务中可供使用的训练数据有限，因而需要对概率分布的参数进行估计，这也是机器学习的核心任务。

![](img/Pasted%20image%2020250321084521.png)

概率的估计有两种方法：最大似然估计法（maximum likelihood estimation）和最大后验概率法（maximum a posteriori estimation），两者分别体现出频率学派和贝叶斯学派对概率的理解方式。

今天我和你分享了人工智能必备的概率论基础，着重于抽象概念的解释而非具体的数学公式，其要点如下：概率论关注的是生活中的不确定性或可能性；频率学派认为先验分布是固定的，模型参数要靠最大似然估计计算；贝叶斯学派认为先验分布是随机的，模型参数要靠后验概率最大化计算；正态分布是最重要的一种随机变量的分布。

## 2.3 数理统计
基础的统计理论有助于对机器学习的算法和数据挖掘的结果做出解释，只有做出合理的解读，数据的价值才能够体现。数理统计（mathematical statistics）根据观察或实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理的估计和判断。

![](img/Pasted%20image%2020250321134457.png)

虽然数理统计以概率论为理论基础，但两者之间存在方法上的本质区别。概率论作用的前提是随机变量的分布已知，根据已知的分布来分析随机变量的特征与规律；数理统计的研究对象则是未知分布的随机变量，研究方法是对随机变量进行独立重复的观察，根据得到的观察结果对原始分布做出推断。

在**统计推断**中，应用的往往不是样本本身，而是被称为统计量的样本的函数。统计量本身是一个随机变量，是用来进行统计推断的工具。**样本均值和样本方差是两个最重要的统计量**：
 + [样本均值](../../700-人工智能/数学知识/统计学/统计基本概念.md#1.1%20样本均值)
 + [样本方差](../../700-人工智能/数学知识/统计学/统计基本概念.md#1.2%20样本方差)

统计推断的基本问题可以分为两大类：参数估计（estimation theory）和假设检验（hypothesis test）。
### 2.3.1 参数估计

**参数估计**是通过随机抽取的样本来估计总体分布的方法，又可以进一步划分为**点估计（point estimation）** 和 **区间估计（interval estimation）**。
在已知总体分布函数形式，但未知其一个或者多个参数时，借助于总体的一个样本来估计未知参数的取值就是参数的点估计。点估计的核心在于构造合适的统计量 $\hat{\theta}$，并用这个统计量的观察值作为未知参数 $\theta$ 的近似值。点估计的具体方法包括**矩估计法（method of moments）** 和**最大似然估计法（maximum likelihood estimation）**。

## 2.4 最优化方法
从本质上讲，**人工智能的目标就是最优化：在复杂环境与多体交互中做出最优决策**。几乎所有的人工智能问题最后都会归结为一个优化问题的求解，因而最优化理论同样是人工智能必备的基础知识。

![](img/Pasted%20image%2020250322093859.png)
### 2.4.1 最优化理论
**最优化理论（optimization）研究的问题是判定给定目标函数的最大值（最小值）是否存在，并找到令目标函数取到最大值（最小值）的数值**。如果把给定的目标函数看成连绵的山脉，最优化的过程就是判断顶峰的位置并找到到达顶峰路径的过程。

要实现最小化或最大化的函数被称为目标函数（objective function）或评价函数，大多数最优化问题都可以通过使目标函数 f(x) 最小化解决，最大化问题则可以通过最小化 −f(x) 实现。

实际的最优化算法既可能找到目标函数的全局最小值（global minimum），也可能找到局部极小值（local minimum），两者的区别在于全局最小值比定义域内所有其他点的函数值都小；而局部极小值只是比所有邻近点的函数值都小。

理想情况下，最优化算法的目标是找到全局最小值。但找到全局最优解意味着在全局范围内执行搜索。还是以山峰做例子。全局最小值对应着山脉中最高的顶峰，找到这个顶峰最好的办法是站在更高的位置上，将所有的山峰尽收眼底，再在其中找到最高的一座。

可遗憾的是，目前实用的最优化算法都不具备这样的上帝视角。它们都是站在山脚下，一步一个脚印地寻找着附近的高峰。但受视野的限制，找到的峰值很可能只是方圆十里之内的顶峰，也就是局部极小值。

当目标函数的输入参数较多、解空间较大时，绝大多数实用算法都不能满足全局搜索对计算复杂度的要求，因而只能求出局部极小值。但在人工智能和深度学习的应用场景下，只要目标函数的取值足够小，就可以把这个值当作全局最小值使用，作为对性能和复杂度的折中。

**根据约束条件的不同，最优化问题可以分为无约束优化（unconstrained optimization）和约束优化（constrained optimization）两类**。无约束优化对自变量 x 的取值没有限制，约束优化则把 x 的取值限制在特定的集合内，也就是满足一定的约束条件。
#### 2.4.1.1 约束优化
**线性规划**（linear programming）就是一类典型的约束优化，其解决的问题通常是在有限的成本约束下取得最大的收益。约束优化问题通常比无约束优化问题更加复杂，但通过**拉格朗日乘子**（Lagrange multiplier）的引入可以将含有 n 个变量和 k 个约束条件的问题转化为含有 (n+k) 个变量的无约束优化问题。拉格朗日函数最简单的形式如下