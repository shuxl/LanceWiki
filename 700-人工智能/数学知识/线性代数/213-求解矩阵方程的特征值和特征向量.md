要理解PCA中特征值和特征向量的作用，首先需要明确它们的数学定义和计算逻辑，再结合具体案例演示。

+ 什么是矩阵的行列式 det(.)
# 1 ​**一、特征值与特征向量的定义**​

对于一个 $n \times n$ 的方阵 $A$，如果存在一个**非零向量**​ $\mathbf{v}$ 和一个**标量**​ $\lambda$，使得：

$$
A\mathbf{v} = \lambda \mathbf{v}
$$

则称：

- $\lambda$ 是矩阵 $A$ 的**特征值**​（Eigenvalue）；
- $\mathbf{v}$ 是对应于 $\lambda$ 的**特征向量**​（Eigenvector）。

# 2 ​**二、计算逻辑：如何求特征值和特征向量？​**​

求解特征值和特征向量的核心步骤是解**特征方程**​：

## 2.1 ​**1. 求特征值**​

特征值是满足以下方程的 $\lambda$：

$$
\det(A - \lambda I) = 0
$$

其中 $I$ 是 $n \times n$ 的单位矩阵，$\det(\cdot)$ 表示矩阵的行列式。这个方程称为**特征多项式**，其根即为所有特征值。

## 2.2 ​**2. 求特征向量**​

对每个特征值 $\lambda_i$，代入方程 $(A - \lambda_i I)\mathbf{v} = 0$，求解非零解向量 $\mathbf{v}$，即为对应的特征向量。

# 3 ​**三、特征值与特征向量的几何意义**​

矩阵 $A$ 可以看作一种**线性变换**​（例如旋转、缩放、投影等）。特征向量 $\mathbf{v}$ 是在线性变换 $A$ 下**方向不变**的向量（可能被缩放），而特征值 $\lambda$ 是该向量被缩放的比例因子：

- 若 $\lambda > 1$：向量被拉长；
- 若 $0 < \lambda < 1$：向量被缩短；
- 若 $\lambda < 0$：向量被反向并缩放；
- 若 $\lambda = 0$：向量被压缩到原点（降维）。

# 4 ​**四、结合三个标准化后的协方差矩阵案例**​

标准化后的协方差矩阵是**相关系数矩阵**​（对角线为1，非对角线为变量间相关系数）。我们分别计算三个案例的特征值和特征向量。

## 4.1 ​**案例1：学习时间与错误率的负相关（标准化后协方差矩阵）​**​

​**标准化后的协方差矩阵**​：

$$
A = \begin{bmatrix}
1 & -1 \\
-1 & 1
\end{bmatrix}
$$

### 4.1.1 ​**步骤1：求特征值**​

解特征方程 $\det(A - \lambda I) = 0$：

$$
\det\left( \begin{bmatrix} 1-\lambda & -1 \\ -1 & 1-\lambda \end{bmatrix} \right) = (1-\lambda)^2 - (-1)(-1) = (1-\lambda)^2 - 1 = 0
$$

展开得：

$$
(1-\lambda)^2 = 1 \implies 1-\lambda = \pm 1 \implies \lambda_1 = 2, \quad \lambda_2 = 0
$$

### 4.1.2 ​**步骤2：求特征向量**​

- 对 $\lambda_1 = 2$：  
    代入 $(A - 2I)\mathbf{v} = 0$：
    
    $$
    \begin{bmatrix} 1-2 & -1 \\ -1 & 1-2 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} -1 & -1 \\ -1 & -1 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = 0
    $$
    


  化简得 $-v_1 - v_2 = 0 \implies v_2 = -v_1$。取 $v_1 = 1$，则特征向量为 $\mathbf{v}_1 = [1, -1]^T$（或其倍数）。  

- 对 $\lambda_2 = 0$：  
  代入 $(A - 0I)\mathbf{v} = 0$：  
$$
\begin{bmatrix} 1 & -1 \ -1 & 1 \end{bmatrix} \begin{bmatrix} v_1 \ v_2 \end{bmatrix} = 0
$$
  化简得 $v_1 - v_2 = 0 \implies v_2 = v_1$。取 $v_1 = 1$，则特征向量为 $\mathbf{v}_2 = [1, 1]^T$（或其倍数）。  


## 4.2 **案例2：身高、体重、鞋码的正相关（标准化后协方差矩阵）**  
**标准化后的协方差矩阵**（假设完全正相关）：  
$$
A = \begin{bmatrix}  
1 & 1 & 1 \\  
1 & 1 & 1 \\  
1 & 1 & 1  
\end{bmatrix}
$$


### 4.2.1 **步骤1：求特征值**  
解特征方程 $\det(A - \lambda I) = 0$。由于矩阵秩为1（所有行相同），非零特征值只有一个，等于矩阵的迹（对角线之和）。  
迹 $\text{tr}(A) = 1+1+1=3$，因此特征值为 $\lambda_1 = 3$，$\lambda_2 = \lambda_3 = 0$（重根）。  


### 4.2.2 **步骤2：求特征向量**  
- 对 $\lambda_1 = 3$：  
  代入 $(A - 3I)\mathbf{v} = 0$：  
$$
\begin{bmatrix} 1-3 & 1 & 1 \\ 1 & 1-3 & 1 \\ 1 & 1 & 1-3 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = \begin{bmatrix} -2 & 1 & 1 \\ 1 & -2 & 1 \\ 1 & 1 & -2 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = 0
$$
  化简得 $-2v_1 + v_2 + v_3 = 0$，$v_1 - 2v_2 + v_3 = 0$，$v_1 + v_2 - 2v_3 = 0$。解得 $v_1 = v_2 = v_3$。取 $v_1 = v_2 = v_3 = 1$，则特征向量为 $\mathbf{v}_1 = [1, 1, 1]^T$（或其倍数）。  

- 对 $\lambda_2 = \lambda_3 = 0$：  
  代入 $A\mathbf{v} = 0$（因为 $A - 0I = A$）：  
$$
\begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = 0
$$
  化简得 $v_1 + v_2 + v_3 = 0$。满足此条件的向量均与 $[1, 1, 1]^T$ 正交，例如 $\mathbf{v}_2 = [1, -1, 0]^T$，$\mathbf{v}_3 = [1, 1, -2]^T$（或其倍数）。  

## 4.3 **案例3：温度、降雨量、股票涨幅的无关（标准化后协方差矩阵）**  
**标准化后的协方差矩阵**（单位矩阵）：  
$$
A = \begin{bmatrix}  
1 & 0 & 0 \\  
0 & 1 & 0 \\  
0 & 0 & 1  
\end{bmatrix}
$$


### 4.3.1 **步骤1：求特征值**  
单位矩阵的特征方程为 $\det(A - \lambda I) = (1-\lambda)^3 = 0$，因此所有特征值均为 $\lambda_1 = \lambda_2 = \lambda_3 = 1$（三重根）。  


### 4.3.2 **步骤2：求特征向量**  
对任意非零向量 $\mathbf{v} = [v_1, v_2, v_3]^T$，有 $A\mathbf{v} = \mathbf{v}$，即 $\lambda = 1$。因此，**所有非零向量都是单位矩阵的特征向量**（对应特征值1）。  


# 5 **五、PCA中特征值与特征向量的作用**  
PCA的核心是通过协方差矩阵（或相关系数矩阵）的特征分解，找到数据的“主方向”（主成分）。具体来说：  

1. **特征向量**：代表主成分的方向。例如，案例1中特征向量 $[1, -1]^T$ 表示“学习时间增加时，错误率减少”的方向；案例2中 $[1, 1, 1]^T$ 表示“身高、体重、鞋码同步增加”的方向。  
2. **特征值**：代表主成分的方差（信息量）。特征值越大，该方向的方差越大，包含的信息越多。例如，案例2中 $\lambda_1 = 3$ 是最大的特征值，对应的特征向量 $[1, 1, 1]^T$ 是第一主成分，解释了大部分数据方差。  


# 6 **总结**  
特征值和特征向量是矩阵的固有属性，描述了矩阵的线性变换特性。在PCA中：  
- 特征向量是主成分的方向；  
- 特征值是主成分的方差（重要性指标）。  
通过选择最大特征值对应的特征向量，PCA可以用最少的维度概括数据的主要信息。
