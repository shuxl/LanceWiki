要理解PCA中的**投影逻辑**，首先需要明确“投影”的数学定义，再结合PCA的核心目标（降维并保留最大信息）推演其过程。以下通过**三个标准化后的协方差矩阵案例**，逐步讲解投影的原理和作用。

# 1 ​**一、什么是投影？​**​

在向量空间中，​**投影**是将一个向量（或点）映射到另一个向量（或子空间）上的操作，类似于“光线垂直照射物体后在地面上的影子”。数学上，若有一个向量 $\mathbf{u}$ 和一个**单位向量** $\mathbf{v}$，则 $\mathbf{u}$ 在 $\mathbf{v}$ 方向上的投影是一个标量（长度）或向量（影子），计算公式为（以下公式只有在v是单位向量时才成立）：

- ​**标量投影**​（长度）：$\text{proj}_{\mathbf{v}} \mathbf{u} = \mathbf{u} \cdot \mathbf{v}$（点积）；
- ​**向量投影**​（影子向量）：$\text{Proj}_{\mathbf{v}} \mathbf{u} = (\mathbf{u} \cdot \mathbf{v}) \mathbf{v}$。

# 2 ​**二、PCA中的投影目标：最大化方差**​

PCA的核心目标是：找到一个**低维子空间**​（通常是1维或2维），将原始高维数据投影到该子空间后，保留尽可能多的信息（即方差）。
- ​**方差的意义**​：方差越大，数据在该方向上的分散程度越高，包含的信息越多；
- ​**投影方向的选择**​：为了保留最大方差，投影方向应选择使得投影后方差最大的方向。数学上，这等价于寻找协方差矩阵 $\Sigma$ 的**最大特征值对应的特征向量**​（主成分方向）。

# 3 ​**三、结合三个案例推演投影逻辑**​

假设原始数据已标准化（均值0，方差1），协方差矩阵 $\Sigma$ 描述了变量间的相关性。我们需要将数据投影到主成分（特征向量）方向上，并计算投影后的方差。

## 3.1 ​**案例1：负相关的二维数据（协方差矩阵 $A = \begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix}$）​**​

​**已知**​：

- 特征值 $\lambda_1 = 2$（最大），对应特征向量 $\mathbf{v}_1 = [1, -1]^T$（单位化后为 $\frac{1}{\sqrt{2}}[1, -1]^T$）；
- 特征值 $\lambda_2 = 0$，对应特征向量 $\mathbf{v}_2 = [1, 1]^T$（单位化后为 $\frac{1}{\sqrt{2}}[1, 1]^T$）。

### 3.1.1 ​**步骤1：原始数据的分布**​

标准化后的数据均值为0，协方差矩阵 $A$ 表示：

- 变量 $x$ 和 $y$ 的方差均为1（对角线元素）；
- $x$ 和 $y$ 的协方差为-1（非对角线元素），说明二者强负相关（$x$ 大时 $y$ 小，反之亦然）。

### 3.1.2 ​**步骤2：投影到主成分方向（$\mathbf{v}_1$）​**​

假设原始数据有一个样本点 $\mathbf{x} = [x_1, x_2]^T$（已标准化），将其投影到 $\mathbf{v}_1$ 方向上的标量投影为：

$$
\text{proj}_{\mathbf{v}_1} \mathbf{x} = \mathbf{x} \cdot \mathbf{v}_1^* = x_1 \cdot \frac{1}{\sqrt{2}} + x_2 \cdot \left(-\frac{1}{\sqrt{2}}\right) = \frac{x_1 - x_2}{\sqrt{2}}
$$

（其中 $\mathbf{v}_1^*$ 是单位化的特征向量）

### 3.1.3 ​**步骤3：投影后的方差**​

投影后的方差等于原始协方差矩阵在 $\mathbf{v}_1$ 方向上的**瑞利商**​（Rayleigh quotient）：

$$
\text{Var}(\text{proj}_{\mathbf{v}_1} \mathbf{x}) = \mathbf{v}_1^* \Sigma \mathbf{v}_1^* = \lambda_1 = 2
$$

这说明投影到 $\mathbf{v}_1$ 方向后，数据的方差为2（远大于原始单个变量的方差1），保留了大部分信息。

### 3.1.4 ​**步骤4：对比投影到次主成分（$\mathbf{v}_2$）​**​

若投影到 $\mathbf{v}_2$ 方向（单位化后），标量投影为：

$$
\text{proj}_{\mathbf{v}_2} \mathbf{x} = \frac{x_1 + x_2}{\sqrt{2}}
$$

投影后的方差为 $\lambda_2 = 0$，说明所有数据点投影后都集中在原点（因为 $\mathbf{v}_2$ 是零方差方向），无信息保留。

## 3.2 ​**案例2：完全正相关的三维数据
协方差矩阵 $B = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$​

​**已知**​：

- 特征值 $\lambda_1 = 3$（最大），对应特征向量 $\mathbf{v}_1 = [1, 1, 1]^T$（单位化后为 $\frac{1}{\sqrt{3}}[1, 1, 1]^T$）；
- 特征值 $\lambda_2 = \lambda_3 = 0$，对应特征向量张成与 $\mathbf{v}_1$ 正交的平面。

### 3.2.1 ​**步骤1：原始数据的分布**​

标准化后的数据中，三个变量 $x, y, z$ 完全正相关（一个变量增大，其他变量同步增大），因此所有数据点大致沿 $\mathbf{v}_1$ 方向分布（例如 $(1,1,1)$、$(2,2,2)$ 等）。

### 3.2.2 ​**步骤2：投影到主成分方向（$\mathbf{v}_1$）​**​

任意样本点 $\mathbf{x} = [x, y, z]^T$ 投影到 $\mathbf{v}_1$ 方向上的标量投影为：

$$
\text{proj}_{\mathbf{v}_1} \mathbf{x} = \frac{x + y + z}{\sqrt{3}}
$$

投影后的方差为 $\lambda_1 = 3$，而原始单个变量的方差为1（标准化后）。由于三个变量完全正相关，投影到 $\mathbf{v}_1$ 方向后，所有方差都被保留（3倍于单个变量），而投影到其他方向（$\lambda_2, \lambda_3$）后方差为0。

### 3.2.3 ​**步骤3：降维的意义**​

原始数据是三维的，但通过投影到 $\mathbf{v}_1$ 方向（一维），即可保留全部方差（3），无需保留其他两个维度（方差为0）。这就是PCA用一维数据概括三维数据的原理。

## 3.3 ​**案例3：无关的二维数据
（协方差矩阵 $C = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$）​**​

​**已知**​：

- 特征值 $\lambda_1 = \lambda_2 = 1$（均为1）；
- 特征向量 $\mathbf{v}_1 = [1, 0]^T$（x轴方向），$\mathbf{v}_2 = [0, 1]^T$（y轴方向）。

### 3.3.1 ​**步骤1：原始数据的分布**​

变量 $x$ 和 $y$ 无关（协方差为0），数据在平面上均匀分布，无主导方向。

### 3.3.2 ​**步骤2：投影到任意主成分方向**​

由于两个特征值相等（均为1），投影到x轴或y轴方向的方差都是1（等于原始单个变量的方差）。此时，选择任意一个特征向量作为主成分均可，因为它们保留的信息量相同。

### 3.3.3 ​**步骤3：降维的效果**​

若选择 $\mathbf{v}_1$（x轴）作为主成分，投影后的数据即为原始x轴数据，方差为1；同理，投影到 $\mathbf{v}_2$（y轴）方差也为1。但由于两个方向方差相同，PCA无法进一步降维（需保留两个维度才能保留全部方差）。

# 4 ​**四、投影的数学本质：线性变换**​

PCA中的投影本质上是一个**线性变换**，由协方差矩阵的特征向量构成的矩阵 $V$（主成分矩阵）完成。假设 $V$ 的列是单位化的特征向量（按特征值从大到小排列），则投影后的数据矩阵 $Z$ 为：

$$
Z = X V
$$

其中 $X$ 是标准化后的原始数据矩阵（每行是样本，每列是变量）。

备注：
+ [219-PCA中投影的数学本质和主成分数量与输出数据形状的关系](219-PCA中投影的数学本质和主成分数量与输出数据形状的关系.md)
# 5 ​**五、总结：投影在PCA中的核心作用**​

1. ​**投影方向**​：由协方差矩阵的特征向量决定，特征向量方向是数据方差最大的方向；
2. ​**投影方差**​：等于对应特征值的大小，特征值越大，投影后保留的信息越多；
3. ​**降维逻辑**​：选择前 $k$ 个最大特征值对应的特征向量，将数据从 $d$ 维降到 $k$ 维，保留约 $80\%-95\%$ 的总方差（具体阈值根据需求设定）。

通过这三个案例可以看出，PCA的投影逻辑本质上是**用数据方差最大的方向作为新坐标轴**，将原始数据映射到这些新坐标轴上，从而用最少的维度概括数据的主要信息。