
# 1 ​**一、问题1：PCA保留1个主成分时，数据矩阵Z是否为n×1的向量？​**​

​**答案：是的。​**​

## 1.1 ​**数学本质：投影是线性变换**​

PCA的核心是将原始数据矩阵 $X$（$n \times d$，$n$ 样本数，$d$ 原始维度）投影到主成分方向上。主成分方向由协方差矩阵 $\Sigma$ 的特征向量构成，这些特征向量组成一个矩阵 $V$（$d \times k$，$k$ 是保留的主成分数量）。

投影后的数据矩阵 $Z$ 由以下公式计算：

$$
Z = X \cdot V
$$

其中：

- $X$ 是标准化后的原始数据矩阵（$n \times d$）；
- $V$ 是主成分矩阵（$d \times k$），每一列是一个主成分的单位化特征向量（按特征值从大到小排列）；
- $Z$ 是投影后的数据矩阵（$n \times k$）。

## 1.2 ​**当 $k=1$ 时的具体情况**​

若只保留1个主成分（$k=1$），则主成分矩阵 $V$ 是 $d \times 1$ 的列向量（记为 $\mathbf{v}_1$）。此时：

$$
Z = X \cdot \mathbf{v}_1
$$

由于 $X$ 是 $n \times d$，$\mathbf{v}_1$ 是 $d \times 1$，矩阵乘法结果 $Z$ 是 $n \times 1$ 的矩阵（即一个列向量）。

## 1.3 ​**案例验证：案例1（二维数据，$k=1$）​**​

原始数据 $X$ 是 $5 \times 2$ 的矩阵（5样本，2变量）：

$$
X = \begin{bmatrix}
-1.26 & 1.45 \\
-0.63 & 0.59 \\
0 & -0.28 \\
0.63 & -0.79 \\
1.26 & -0.97
\end{bmatrix}
$$

主成分矩阵 $V$ 是 $2 \times 1$ 的列向量（对应最大特征值 $\lambda_1=2$ 的特征向量 $\mathbf{v}_1 = \frac{1}{\sqrt{2}}[1, -1]^T$）：

$$
V = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix}
$$

投影后的 $Z$ 为：

$$
Z = X \cdot V = \frac{1}{\sqrt{2}} \begin{bmatrix}
-1.26 \times 1 + 1.45 \times (-1) \\
-0.63 \times 1 + 0.59 \times (-1) \\
0 \times 1 + (-0.28) \times (-1) \\
0.63 \times 1 + (-0.79) \times (-1) \\
1.26 \times 1 + (-0.97) \times (-1)
\end{bmatrix} = \frac{1}{\sqrt{2}} \begin{bmatrix} -2.71 \\ -1.22 \\ 0.28 \\ 1.42 \\ 2.23 \end{bmatrix}
$$

结果 $Z$ 是 $5 \times 1$ 的列向量，每个元素是样本在主成分方向上的投影值。

# 2 ​**二、问题2：PCA保留k个主成分时，数据矩阵Z是否为n×k的矩阵？每列是X与V的线性变换？​**​

​**答案：是的。​**​

## 2.1 ​**数学本质：多维投影**​

当保留 $k$ 个主成分时，主成分矩阵 $V$ 是 $d \times k$ 的矩阵（$k$ 列，每列是一个主成分的单位化特征向量）。此时，投影后的数据矩阵 $Z$ 是 $n \times k$ 的矩阵，其中：

- 每一行对应一个原始样本；
- 每一列对应一个主成分方向的投影值。

## 2.2 ​**当 $k=2$ 时的具体情况（以案例1为例）​**​

若保留2个主成分（$k=2$），主成分矩阵 $V$ 是 $2 \times 2$ 的矩阵（包含前2个最大特征值的特征向量）：

$$
V = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix}
$$

（第一列是 $\mathbf{v}_1 = \frac{1}{\sqrt{2}}[1, -1]^T$，第二列是 $\mathbf{v}_2 = \frac{1}{\sqrt{2}}[1, 1]^T$，对应特征值 $\lambda_1=2$ 和 $\lambda_2=0$）。

投影后的 $Z$ 为：

$$
Z = X \cdot V = \frac{1}{\sqrt{2}} \begin{bmatrix}
-1.26 & 1.45 \\
-0.63 & 0.59 \\
0 & -0.28 \\
0.63 & -0.79 \\
1.26 & -0.97
\end{bmatrix} \cdot \begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix}
$$

计算后得到：

$$
Z = \frac{1}{\sqrt{2}} \begin{bmatrix}
(-1.26 \times 1 + 1.45 \times (-1)) & (-1.26 \times 1 + 1.45 \times 1) \\
(-0.63 \times 1 + 0.59 \times (-1)) & (-0.63 \times 1 + 0.59 \times 1) \\
(0 \times 1 + (-0.28) \times (-1)) & (0 \times 1 + (-0.28) \times 1) \\
(0.63 \times 1 + (-0.79) \times (-1)) & (0.63 \times 1 + (-0.79) \times 1) \\
(1.26 \times 1 + (-0.97) \times (-1)) & (1.26 \times 1 + (-0.97) \times 1)
\end{bmatrix}
$$

$$
= \frac{1}{\sqrt{2}} \begin{bmatrix}
-2.71 & 0.19 \\
-1.22 & -0.04 \\
0.28 & -0.28 \\
1.42 & -0.16 \\
2.23 & 0.29
\end{bmatrix}
$$

结果 $Z$ 是 $5 \times 2$ 的矩阵：

- 第一列是样本在 $\mathbf{v}_1$ 方向上的投影（方差为2，保留主要信息）；
- 第二列是样本在 $\mathbf{v}_2$ 方向上的投影（方差为0，所有数据点投影后集中在原点附近）。

# 3 ​**三、总结：Z的形状与主成分数量的关系**​

|主成分数量 $k$|原始数据 $X$ 形状|主成分矩阵 $V$ 形状|投影后数据 $Z$ 形状|含义|
|---|---|---|---|---|
|$k=1$|$n \times d$|$d \times 1$|$n \times 1$|所有样本投影到1个主成分方向，结果为1维向量|
|$k=2$|$n \times d$|$d \times 2$|$n \times 2$|所有样本投影到2个主成分方向，结果为2维矩阵（每列对应1个主成分）|

# 4 ​**关键结论**​

- ​**投影的数学本质**​：PCA的投影是通过矩阵乘法实现的线性变换，将原始数据 $X$ 映射到主成分方向上，结果 $Z$ 的形状由主成分数量 $k$ 决定（$n \times k$）。
- ​**主成分数量与Z的形状**​：
    - $k=1$ 时，$Z$ 是 $n \times 1$ 的向量（1维投影）；
    - $k>1$ 时，$Z$ 是 $n \times k$ 的矩阵（每列对应1个主成分的投影）。

通过这个过程，你可以清晰理解：PCA的投影逻辑是通过特征向量矩阵 $V$ 将原始数据映射到主成分空间，最终数据矩阵 $Z$ 的形状直接反映了保留的主成分数量。