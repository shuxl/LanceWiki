**梯度下降（Gradient Descent）** 是机器学习中最常用的优化算法之一，用来**最小化损失函数**，从而找到模型的最佳参数。

你可以把它理解为：
> 在一个“山谷”形状的函数图像中，我们从某个起点开始，一步步“沿着最陡的下坡方向”走，直到到达最低点。

#算法/梯度下降法

---

# 1 **一、梯度下降的目标**

在训练模型（如逻辑回归、神经网络）时，我们有一个**损失函数** $L(\theta)$，它表示“模型预测值和真实值之间的误差”。

目标是：
$\min_\theta L(\theta)$

也就是找出参数 $\theta$ 使得损失函数最小。

---

# 2 **二、梯度的定义和几何意义**

- 梯度（gradient）是一个向量，记作 $\nabla_\theta L(\theta)$，表示“损失函数在当前参数位置增长最快的方向”。
- 因此，负梯度方向（-∇）是“函数下降最快的方向”。

---

# 3 **三、梯度下降的更新公式**

基本更新规则如下：
$\theta \leftarrow \theta - \eta \cdot \nabla_\theta L(\theta)$
- $\theta$：模型参数（可以是一组向量）
- $\eta$：**学习率（learning rate）**，表示每次走多大一步
- $\nabla_\theta L(\theta)$：当前参数位置的梯度

每次迭代，沿着负梯度方向走一步，损失就会下降一点。

---

# 4 **四、图像理解（二维示意）**

想象一个碗状的函数图像，最低点就是最优参数位置：

```
      L(θ)
       ▲
       │       ●← 第三步（继续下降）
       │     ●
       │   ●
       │ ●
       └────────────────► θ（参数）
        起点 →→→→→ 最优值
```

每次都朝着“最陡的下坡”走一点，最终走到最低点。

---

# 5 **五、梯度下降的变种**
## 5.1 **1.批量梯度下降（Batch GD）**

- 每次用所有训练数据来计算梯度
- 精确但慢，内存开销大
## 5.2 **2.随机梯度下降（SGD）**

- 每次只用一个样本更新梯度
- 快速、但波动大

## 5.3 **3.小批量梯度下降（Mini-batch SGD）**

- 每次用一个小批量样本（如 32、64）计算梯度
- 在效率和稳定性之间折中，最常用

## 5.4 **带动量的梯度下降（Momentum）**

- 类似惯性，避免震荡，加快收敛

## 5.5 **自适应学习率方法**

- 如 Adam、RMSprop、Adagrad，让不同参数有不同学习率，效果更好

---

# 6 **六、一个简单例子（用平方误差）**

假设你在训练一个模型：
$L(\theta) = (\theta - 3)^2$
这个函数图像是一个以 3 为最低点的抛物线，我们从初始值 $\theta = 0$ 开始：
$\nabla_\theta L(\theta) = 2(\theta - 3)$
用学习率 $\eta = 0.1$：
第一步：
$\theta = 0 - 0.1 \cdot 2(0 - 3) = 0 + 0.6 = 0.6$
第二步：
$\theta = 0.6 - 0.1 \cdot 2(0.6 - 3) = 0.6 + 0.48 = 1.08$
……
不断迭代，$\theta$ 会慢慢逼近 3。

---

# 7 **七、总结**

| **项目** | **内容**                                                          |
| ------ | --------------------------------------------------------------- |
| 作用     | 最小化损失函数，优化模型参数                                                  |
| 原理     | 沿着梯度的反方向更新参数                                                    |
| 核心公式   | $\theta \leftarrow \theta - \eta \cdot \nabla_\theta L(\theta)$ |
| 常用变种   | SGD、Mini-batch、Momentum、Adam                                    |
| 关键参数   | 学习率 $\eta$，太大不收敛，太小太慢                                           |
# 8 其它例子
我们来举一个非常常见、简单但清晰的例子：**一元线性回归**，用梯度下降一步步找到最优参数。

---

## 8.1 **场景描述**

我们想要拟合一个模型：

$\hat{y} = \theta_0 + \theta_1 x$
已知的数据：

|**x**|**y**|
|---|---|
|1|2|
|2|3|
|3|4|

可以看出这是一条近似的直线，我们要找到合适的 \theta_0 和 \theta_1，使得预测值 \hat{y} 尽量接近真实值 y。

---

## 8.2 **一、损失函数：MSE（均方误差）**

我们定义损失函数为：
$L(\theta_0, \theta_1) = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}i - y_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (\theta_0 + \theta_1 x_i - y_i)^2$

目标就是最小化这个函数。

---

## 8.3 **二、对参数求偏导（梯度）**

我们需要计算每一轮要更新多少，使用偏导数：[^1]

$\frac{\partial L}{\partial \theta_0} = \frac{2}{n} \sum_{i=1}^{n} (\theta_0 + \theta_1 x_i - y_i)$

$\frac{\partial L}{\partial \theta_1} = \frac{2}{n} \sum_{i=1}^{n} (\theta_0 + \theta_1 x_i - y_i) \cdot x_i$


---

## 8.4 **三、梯度下降的更新公式**

每次更新参数如下：

$\theta_0 \leftarrow \theta_0 - \eta \cdot \frac{\partial L}{\partial \theta_0}$

$\theta_1 \leftarrow \theta_1 - \eta \cdot \frac{\partial L}{\partial \theta_1}$

其中 $\eta$ 是学习率，我们设为 0.1。

---

## 8.5 **四、用 Python 实现这个过程（手动梯度下降）**

```
# 数据
x = [1, 2, 3]
y = [2, 3, 4]
n = len(x)

# 初始化参数
theta0 = 0
theta1 = 0

# 学习率
eta = 0.1

# 迭代次数
epochs = 100

for epoch in range(epochs):
    # 计算预测值和误差
    y_hat = [theta0 + theta1 * xi for xi in x]
    error = [y_hat[i] - y[i] for i in range(n)]

    # 计算梯度
    grad_theta0 = 2 * sum(error) / n
    grad_theta1 = 2 * sum(error[i] * x[i] for i in range(n)) / n

    # 更新参数
    theta0 -= eta * grad_theta0
    theta1 -= eta * grad_theta1

    # 打印每轮结果
    print(f"Epoch {epoch+1}: theta0 = {theta0:.4f}, theta1 = {theta1:.4f}")
```

---

## 8.6 **五、输出示例（简化）**

```
Epoch 1: theta0 = 0.6, theta1 = 1.33
Epoch 2: theta0 = 0.94, theta1 = 0.87
Epoch 3: theta0 = 1.13, theta1 = 0.67
...
Epoch 100: theta0 ≈ 1.0, theta1 ≈ 1.0
```

结果逐渐逼近 $\theta_0$ = 1, $\theta_1$ = 1，也就是拟合出 $\hat{y} = x + 1$，很符合原始数据的规律。

---

## 8.7 **六、总结**

这个例子展示了：
- 如何通过每轮计算误差、求梯度，来“爬下山”；
- 梯度方向告诉你怎么调整参数；
- 每次沿着负梯度走一步，损失越来越小，模型越来越好。

[^1]: 这里用到了链式法则： [230-链式法则](230-链式法则.md)
