

# 1 一、张量创建方法

1. ​**基础创建函数**​
    
    ```
    # 直接创建
    t1 = torch.tensor([[1, 2], [3, 4]])
    
    # 全零张量
    zeros = torch.zeros(2, 3)  # 2行3列
    
    # 全一张量
    ones = torch.ones(3)       # 形状(3,)
    
    # 等差张量
    range_t = torch.arange(0, 5, 1)  # [0, 1, 2, 3, 4]
    ```
    

# 2 二、核心张量操作

1. ​**维度查看**​
    
    ```
    t = torch.rand(2, 3, 4)
    print(t.shape)  # 输出: torch.Size([2, 3, 4])
    ```
    
2. ​**形状变换**​

| 方法            | 特点               | 示例                  |
| ------------- | ---------------- | ------------------- |
| `view()`      | 要求内存连续  返回原始数据视图 | `x.view(2, 3)`      |
| `reshape()`   | 自动处理连续性  必要时创建副本 | `x.reshape(3, -1)`  |
| `transpose()` | 维度交换  常导致不连续     | `x.transpose(0, 1)` |

```
# 实际应用
x = torch.arange(6)  # [0,1,2,3,4,5]
x_view = x.view(2, 3)  # [[0,1,2],[3,4,5]]
x_trans = x_view.transpose(0, 1)  # [[0,3],[1,4],[2,5]]
```
    

# 3 三、内存连续性深度解析

## 3.1 内存连续性是什么？

- ​**连续内存**​：元素在物理内存中顺序存储
- ​**非连续内存**​：逻辑形状与物理存储顺序不匹配
- ​**关键指标**​：`stride`（步幅）决定维度遍历时的内存跳跃距离

## 3.2 导致内存不连续的常见操作：

1. ​**维度变换**​
    
    - `.t()`（转置）
    - `.transpose()`
    - `.permute()`
    
    ```
    x = torch.rand(3, 4)
    y = x.t()  # 转置 -> 不连续
    print(y.is_contiguous())  # False
    ```
    
2. ​**特殊切片**​
    
    ```
    z = x[::2, :]  # 步长为2的切片 -> 不连续
    ```
    
3. ​**广播扩展**​
    
    ```
    a = torch.arange(3)
    b = a.expand(3, 3)  # 广播复制 -> 不连续
    ```
    
4. ​**特殊构造**​
    
    ```
    # 自定义步幅创建
    c = torch.as_strided(x, size=(2,2), stride=(1,2))
    ```
    
5. ​**NumPy转换**​
    
    ```
    import numpy as np
    arr = np.random.rand(4,3).T  # NumPy非连续数组
    d = torch.from_numpy(arr)   # PyTorch非连续张量
    ```
    
6. ​**拼接操作**​
    
    ```
    # 拼接操作本身产生连续结果
    a = torch.rand(2,3)
    b = torch.rand(2,3).t()  # 非连续
    c = torch.cat([a, b], dim=0)
    
    # 但输入的不连续性会影响性能
    ```
    

## 3.3 拼接操作的特殊性：

- ​**结果总是连续的**​：拼接会创建新内存块
- ​**性能隐患**​：输入的不连续性会导致额外复制开销
    
    ```
    # 拼接过程隐含步骤：
    # 1. 为结果分配连续内存
    # 2. 复制非连续输入到新内存
    # 3. 复制连续输入到新内存
    ```
    

# 4 四、诊断与修复方法

1. ​**诊断工具**​
    
    ```
    # 检查连续性
    tensor.is_contiguous()
    
    # 查看步幅
    tensor.stride()  # 例如 (1, 3) 表示非连续
    ```
    
2. ​**修复方法**​
    
    ```
    # 创建连续副本
    contiguous_tensor = tensor.contiguous()
    
    # 或通过复制
    clone_tensor = tensor.clone()  # 自动创建连续副本
    ```
    

# 5 五、性能优化建议

1. ​**形状变换选择**​
    
|场景|推荐方法|
|---|---|
|确定连续时|`view()`（高效）|
|不确定时|`reshape()`（安全）|
|需处理维度|`transpose()`+`contiguous()`|
    
2. ​**数据处理最佳实践**​
    
    ```
    # 推荐流程：
    # 1. 输入时连续化
    input_data = input_data.contiguous()
    
    # 2. 操作时避免中间不连续
    # 3. 输出通常自动连续
    ```
    
3. ​**拼接操作优化**​
    
    ```
    # 提前连续化输入
    a = torch.rand(2,3)
    b = torch.rand(3,2).t().contiguous()  # 提前处理
    c = torch.cat([a, b])
    ```
    

# 6 六、核心总结

1. ​**内存连续性本质**​
    
    - 逻辑形状与实际内存布局的关系
    - 通过`stride`控制元素访问模式
2. ​**关键区分**​
    
|特性|`view()`|`reshape()`|`transpose()`|
|---|---|---|---|
|数据复制|×|△(可能)|×|
|连续性要求|严格|无|导致不连续|
|返回类型|视图|视图或副本|视图|
    
3. ​**性能关键点**​
    
    - 避免在训练循环中出现意外不连续
    - 拼接前预处理非连续输入
    - GPU操作需要连续内存
    - 对大数据使用`reshape()`更安全

理解内存连续性机制可显著提升PyTorch程序的效率和稳定性，特别是在处理高维数据和大规模模型时。