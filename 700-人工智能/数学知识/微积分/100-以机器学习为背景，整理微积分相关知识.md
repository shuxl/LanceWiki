**带着机器学习的实际问题去学微积分**。下面我以「你在机器学习中会遇到的典型问题」为主线，带你掌握相关的微积分知识。
+ 其它知识
	+ [102-概念：预测、估计、或拟合值](102-概念：预测、估计、或拟合值.md)
	+ [230-链式法则](230-链式法则.md)
	+ [103-Sigmoid函数](103-Sigmoid函数.md)
	+ [104-Sigmiod中的向量点积（内积）](104-Sigmiod中的向量点积（内积）.md)
	+ [107-损失函数](107-损失函数.md)

---

# 1 **总体框架：机器学习常见流程 vs 微积分知识**

| **机器学习步骤** | **遇到的问题** | **需要的微积分知识**    |
| ---------- | --------- | --------------- |
| 1. 构造损失函数  | 如何量化预测误差？ | 函数的基本形式和导数      |
| 2. 求解最优参数  | 如何让误差最小？  | 一阶导数（梯度），极值点    |
| 3. 梯度下降    | 如何更新参数？   | 梯度的概念、链式法则      |
| 4. 加速收敛    | 用牛顿法更快收敛？ | 二阶导数、Hessian 矩阵 |
| 5. 函数凸性判断  | 解是否是全局最优？ | 凸函数判别（Hessian）  |

我们下面按这些步骤，边讲微积分知识，边解释它在 ML 中的作用。

---

# 2 **一、损失函数与一阶导数：从逻辑回归开始**

## 2.1 **场景：**

你在用逻辑回归解决一个二分类问题，损失函数是：
$L(\theta) = -\sum_{i=1}^{n} \left[ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \right]$
其中 $\sigma$：
- $\hat{y}_i = \sigma(\theta^T x_i) = \frac{1}{1 + e^{-\theta^T x_i}}$
## 2.2 **想解决的问题：**

> 怎么最小化损失函数？每次该怎么调整参数 $\theta$？
## 2.3 **所需微积分知识：**

### 2.3.1 导数的定义：衡量函数在某点的变化率
- 如果：
    $f(x) = x^2 \Rightarrow f’(x) = 2x$
- 意义：在当前点，向哪个方向变动，函数值减少得最快？

### 2.3.2 链式法则：

- 用于处理复合函数（比如 sigmoid 函数）：
    $\frac{d}{d\theta} \sigma(\theta^T x) = \sigma(\theta^T x)(1 - \sigma(\theta^T x)) x$

---

# 3 **二、梯度与梯度下降法（Gradient Descent）**

## 3.1 **场景：**
你已经知道损失函数 $L(\theta)$，需要更新参数：

$\theta^{(t+1)} = \theta^{(t)} - \eta \cdot \nabla_\theta L(\theta)$

## 3.2 **所需微积分知识：**

### 3.2.1 梯度（Gradient）：

- 多变量函数的导数扩展：
    $\nabla_\theta L(\theta) = \left[ \frac{\partial L}{\partial \theta_1}, \cdots, \frac{\partial L}{\partial \theta_n} \right]$
- 意义：函数增长最快方向的“指示器”。
### 3.2.2 几何解释：

- 梯度指向“上坡”方向；
- 负梯度（-∇）就是“下坡”方向，用于最小化目标函数。

---

# 4 **三、牛顿法和 Hessian 矩阵：更快找到最优解**

## 4.1 **场景：**
你发现梯度下降很慢，想加速收敛。
牛顿法的更新公式是：

$\theta^{(t+1)} = \theta^{(t)} - H^{-1} \nabla_\theta L(\theta)$

其中 H 是 Hessian 矩阵。

## 4.2 **所需微积分知识：**

### 4.2.1 二阶导数：

- 比如 $f(x) = x^2$，一阶导数是 $f’(x) = 2x$，二阶导数是 $f’’(x) = 2$
### 4.2.2 Hessian 矩阵：

- 是二阶偏导数组成的矩阵，描述函数的“曲率”：
    $H(f) = \left[ \frac{\partial^2 f}{\partial \theta_i \partial \theta_j} \right]$
### 4.2.3 优化用途：

- 用 Hessian 来调整梯度方向，使得下降“更聪明”，不瞎试探。
- 比如 L-BFGS 算法中，会近似 Hessian，避免存整个矩阵。

---

# 5 **四、极值判断与凸函数场景：**

你希望模型找到的是**全局最优解**，不是卡在局部极小值。

## 5.1 **所需微积分知识：**

### 5.1.1 临界点（Critical point）：

- 梯度为零的点：
    $\nabla f(\theta) = 0$
- 是极值点候选，但不一定是极小值！
### 5.1.2 **2.用 Hessian 判别极值性质：**

- 若 Hessian 正定（所有特征值 > 0），该点是**局部最小值**；
- 若 Hessian 非正定，可能是鞍点。

### 5.1.3 **3.凸函数定义：**

- 若函数的 Hessian 在所有点都为正半定，那么函数是**凸的**；
- 对凸函数来说，任何局部最小值即是全局最小值。

---

# 6 **五、机器学习中常见的函数微积分要求汇总**

| **典型函数**                                                | **用途**       | **微积分内容** |
| ------------------------------------------------------- | ------------ | --------- |
| $f(x) = x^2$                                            | 学习最小值、导数     | 基本导数规则    |
| $f(x) = \log x$                                         | 损失函数中常见      | 链式法则、导数   |
| $\sigma(x) = \frac{1}{1+e^{-x}}$                        | Logistic 回归中 | 链式法则、一阶导  |
| $L(\theta) = -y\log \hat{y} - (1 - y)\log(1 - \hat{y})$ | 交叉熵损失        | 多元导数、链式法则 |
| $H(f)$                                                  | 牛顿法、极值判断     | 二阶导数矩阵    |

---

# 7 **结语：如何学习这些内容？**

建议你结合下面的路线逐步掌握：
1. **先学**：导数、偏导数、链式法则；
2. **再学**：梯度、梯度下降；
3. **进阶**：二阶导数、Hessian、牛顿法；
4. **补充**：凸函数理论、优化收敛性分析。
