在机器学习中，公式中的 \sigma 通常表示的是 **Sigmoid 函数**，也称为 **逻辑函数**。这是一个非常重要的激活函数，尤其常见于逻辑回归（Logistic Regression）和神经网络中。

+ 知识点
	+ $\theta^T x$ ：[104-Sigmiod中的向量点积（内积）](104-Sigmiod中的向量点积（内积）.md)
	+ Sigmoid函数求导的公式： [106-Sigmoid 函数 求导](106-Sigmoid%20函数%20求导.md)
	+ 反向传播 [105-反向传播 的 理解](105-反向传播%20的%20理解.md)

---

# 1 **一、Sigmoid 函数的定义**

$\sigma(z) = \frac{1}{1 + e^{-z}}$

其中 z 是实数，可以是模型的线性组合输出，比如：

$z = \theta^T x = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n$

---

# 2 **二、函数图像和性质**

## 2.1 **图像形状：**

- Sigmoid 是一个 **S 形曲线（S-shaped curve）**；
- 输出值在 (0, 1) 之间；
- 当 z = 0 时，$\sigma(z) = 0.5$；
- 当 $z \to +\infty，\sigma(z) \to 1$；
- 当 $z \to -\infty，\sigma(z) \to 0$。

## 2.2 **图像说明：**

```
   σ(z)
    ↑
  1 |                         ●
    |                      ●
0.5 |-----------●-----------
    |       ●
  0 |●--------------------------→ z
       -∞      0        +∞
```

---

# 3 **三、在机器学习中的作用**

## 3.1 逻辑回归中用于概率建模：
逻辑回归模型预测的不是 0 或 1，而是“属于某一类的概率”，这个概率就是用 sigmoid 函数转化而来：
$P(y=1|x) = \sigma(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}}$

## 3.2 二分类模型的输出：

- 如果 $\sigma(\theta^T x) > 0.5$，预测为正类；
- 如果 < 0.5，预测为负类。

## 3.3 神经网络中的激活函数（较早期的用法）：

- 给神经元加上非线性变换；
- 现代中 ReLU 使用更广泛，但 sigmoid 在输出层依然常见（尤其是二分类任务）。

---

# 4 **四、导数公式（用于反向传播）**
Sigmoid 函数的导数是：

$\sigma’(z) = \sigma(z)(1 - \sigma(z))$

这个形式非常简洁，利于神经网络的训练和反向传播。
